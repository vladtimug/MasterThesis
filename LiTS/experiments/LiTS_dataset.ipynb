{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndi\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 131 training samples at ../../../unet-lits-2d-pipeline/OriginalData/Training_Data/\n",
      "Found 70 training samples at ../../../unet-lits-2d-pipeline/OriginalData/Test_Data/\n"
     ]
    }
   ],
   "source": [
    "ROOT_RAW_TRAINING_DATA_PATH = \"../../../unet-lits-2d-pipeline/OriginalData/Training_Data/\"\n",
    "ROOT_RAW_TEST_DATA_PATH = \"../../../unet-lits-2d-pipeline/OriginalData/Test_Data/\"\n",
    "\n",
    "print(f\"Found {len(os.listdir(ROOT_RAW_TRAINING_DATA_PATH)) // 2} training samples at {ROOT_RAW_TRAINING_DATA_PATH}\")\n",
    "print(f\"Found {len(os.listdir(ROOT_RAW_TEST_DATA_PATH))} training samples at {ROOT_RAW_TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PREPROCESSED_TRAINING_DATA_PATH = \"../../../unet-lits-2d-pipeline/LOADDATA/Training_Data_2D/\"\n",
    "ROOT_PREPROCESSED_TEST_DATA_PATH = \"../../../unet-lits-2d-pipeline/LOADDATA/Test_Data_2D/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Basic Dataset Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Hounsfield voxel value for tissue. Any value smaller than this is conventinally thought to correspond to water.\n",
    "MIN_BOUND = -100\n",
    "\n",
    "# Maximum Hounsfield voxel value for tissue. Any value greater than this is conventionally thought to correspond to bone structure.\n",
    "MAX_BOUND = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LiTS dataset mean and standard deviation\n",
    "# test_volumes = os.listdir(ROOT_RAW_TEST_DATA_PATH)\n",
    "# training_volumes = os.listdir(ROOT_RAW_TRAINING_DATA_PATH)\n",
    "# training_volumes = [elem for elem in training_volumes if \"volume\" in elem]\n",
    "# dataset_volumes = training_volumes + test_volumes\n",
    "\n",
    "# volume_means = []\n",
    "# volume_stds = []\n",
    "\n",
    "# for volume_index, volume_name in enumerate(dataset_volumes):\n",
    "#     root_path = \"\"\n",
    "#     if volume_name in test_volumes:\n",
    "#         root_path = ROOT_RAW_TEST_DATA_PATH\n",
    "#     elif volume_name in training_volumes:\n",
    "#         root_path = ROOT_RAW_TRAINING_DATA_PATH\n",
    "\n",
    "#     volume = nib.load(root_path + volume_name)\n",
    "\n",
    "#     volume_data = volume.get_fdata()\n",
    "#     min_voxel_value, max_voxel_value = volume_data.min(), volume_data.max()\n",
    "#     normalized_volume_data = (volume_data - min_voxel_value) / (max_voxel_value - min_voxel_value)\n",
    "    \n",
    "#     normalized_volume_mean = np.mean(normalized_volume_data)\n",
    "#     volume_means.append(normalized_volume_mean)\n",
    "    \n",
    "#     normalized_volume_std = np.sqrt(np.sum((normalized_volume_data - normalized_volume_mean) ** 2) / normalized_volume_data.size)\n",
    "#     volume_stds.append(normalized_volume_std)\n",
    "\n",
    "# DATASET_MEAN = np.mean(np.array(volume_means))\n",
    "# DATASET_STD = np.mean(np.array(volume_stds))\n",
    "\n",
    "# print(DATASET_MEAN)\n",
    "# print(DATASET_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUALLY_COMPUTED_DATASET_MEAN = 0.1572\n",
    "MANUALLY_COMPUTED_DATASET_STD = 0.14909\n",
    "\n",
    "# Expected values on the LiTS dataset\n",
    "DATASET_MEAN = 0.1021\n",
    "DATASET_STD = 0.19177"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"======================================\"\"\"\n",
    "\"\"\"========== Basic Utilities ===========\"\"\"\n",
    "\"\"\"======================================\"\"\"\n",
    "def set_bounds(image,MIN_BOUND,MAX_BOUND):\n",
    "    \"\"\"\n",
    "    Clip image to lower bound MIN_BOUND, upper bound MAX_BOUND.\n",
    "    \"\"\"\n",
    "    return np.clip(image,MIN_BOUND,MAX_BOUND)\n",
    "\n",
    "def normalize(image,use_bd=True,zero_center=True,unit_variance=True,supply_mode=\"orig\"):\n",
    "    \"\"\"\n",
    "    Perform standardization/normalization, i.e. zero_centering and Setting\n",
    "    the data to unit variance.\n",
    "    Input Arguments are self-explanatory except for:\n",
    "    supply_mode: Describes the type of LiTS-Data, i.e. whether it has been\n",
    "                 rescaled/resized or not. See >Basic_Parameter_Values<\n",
    "    \"\"\"\n",
    "    if not use_bd:\n",
    "        MIN_BOUND = np.min(image)\n",
    "        MAX_BOUND = np.max(image)\n",
    "    else:\n",
    "        MIN_BOUND = -100.0 #Everything below: Water\n",
    "        MAX_BOUND = 400.0\n",
    "        image = set_bounds(image,MIN_BOUND,MAX_BOUND)\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image = np.clip(image,0.,1.)\n",
    "    \n",
    "    # TODO: Figure out how the mean and std values are computed\n",
    "    if zero_center:\n",
    "        image = image - DATASET_MEAN\n",
    "    if unit_variance:\n",
    "        image = image/DATASET_STD\n",
    "    return image\n",
    "\n",
    "\"\"\"======================================\"\"\"\n",
    "\"\"\"============ Augmentation ============\"\"\"\n",
    "\"\"\"======================================\"\"\"\n",
    "##############################################################################################\n",
    "def rotate_2D(to_aug, rng=np.random.RandomState(1)):\n",
    "    \"\"\"\n",
    "    Perform standard 2D-per-slice image rotation.\n",
    "    Arguments:\n",
    "    to_aug:     List of files that should be deformed in the same way. Each element\n",
    "                must be of standard Torch_Tensor shape: (C,W,H,...).\n",
    "                Deformation is done equally for each channel, but differently for\n",
    "                each image in a batch if N!=1.\n",
    "    rng:        Random Number Generator that can be provided for the Gaussian filter means.\n",
    "    copy_files: If True, copies the input files before transforming. Ensures that the actual\n",
    "                input data remains untouched. Otherwise, it is directly altered.\n",
    "\n",
    "    Function only returns data when copy_files==True.\n",
    "    \"\"\"\n",
    "    angle = (rng.rand()*2-1)*10\n",
    "    for i,aug_file in enumerate(to_aug):\n",
    "        for ch in range(aug_file.shape[0]):\n",
    "            #actually perform rotation\n",
    "            aug_file[ch,:]    = ndi.rotate(aug_file[ch,:].astype(np.float32), angle, reshape=False, order=0, mode=\"nearest\")\n",
    "    return to_aug, angle\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "def zoom_2D(to_aug, rng=np.random.RandomState(1)):\n",
    "    \"\"\"\n",
    "    Perform standard 2D per-slice zooming/rescaling.\n",
    "    Arguments:\n",
    "    to_aug:     List of files that should be deformed in the same way. Each element\n",
    "                must be of standard Torch_Tensor shape: (N,C,W,H,...).\n",
    "                Deformation is done equally for each channel, but differently for\n",
    "                each image in a batch if N!=1.\n",
    "    rng:        Random Number Generator that can be provided for the Gaussian filter means.\n",
    "    copy_files: If True, copies the input files before transforming. Ensures that the actual\n",
    "                input data remains untouched. Otherwise, it is directly altered.\n",
    "\n",
    "    Function only returns data when copy_files==True.\n",
    "    Note: Should also work for 3D, but has not been tested for that.\n",
    "    \"\"\"\n",
    "    # TODO: Figure out how the magnification range limits are computed\n",
    "    magnif = rng.uniform(0.825,1.175)\n",
    "    for i,aug_file in enumerate(to_aug):\n",
    "        for ch in range(aug_file.shape[0]):\n",
    "            sub_img     = aug_file[ch,:]\n",
    "            # sub_mask    = aug_file[ch,:]\n",
    "            img_shape   = np.array(sub_img.shape)\n",
    "            new_shape   = [int(np.round(magnif*shape_val)) for shape_val in img_shape]\n",
    "            zoomed_shape= (magnif,)*(sub_img.ndim)\n",
    "\n",
    "            if magnif<1:\n",
    "                how_much_to_clip    = [(x-y)//2 for x,y in zip(img_shape, new_shape)]\n",
    "                idx_cornerpix       = tuple(-1 for _ in range(sub_img.ndim))\n",
    "                idx_zoom            = tuple(slice(x,x+y) for x,y in zip(how_much_to_clip,new_shape))\n",
    "                zoomed_out_img      = np.ones_like(sub_img)*sub_img[idx_cornerpix]\n",
    "                zoomed_out_img[idx_zoom] = ndi.zoom(sub_img.astype(np.float32),zoomed_shape,order=0,mode=\"nearest\")\n",
    "                aug_file[ch,:]        = zoomed_out_img\n",
    "\n",
    "            if magnif>1:\n",
    "                zoomed_in_img       = ndi.zoom(sub_img.astype(np.float32),zoomed_shape,order=0,mode=\"nearest\")\n",
    "                rounding_correction = [(x-y)//2 for x,y in zip(zoomed_in_img.shape,img_shape)]\n",
    "                rc_idx              = tuple(slice(x,x+y) for x,y in zip(rounding_correction, img_shape))\n",
    "                aug_file[ch,:]   = zoomed_in_img[rc_idx]\n",
    "\n",
    "    return to_aug\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "def hflip_2D(to_aug, rng=np.random.RandomState(1)):\n",
    "    \"\"\"\n",
    "    Perform standard 2D per-slice horizontal_flipping.\n",
    "    Arguments:\n",
    "    to_aug:     List of files that should be deformed in the same way. Each element\n",
    "                must be of standard Torch_Tensor shape: (N,C,W,H,...).\n",
    "                Deformation is done equally for each channel, but differently for\n",
    "                each image in a batch if N!=1.\n",
    "    rng:        Random Number Generator that can be provided for the Gaussian filter means.\n",
    "    copy_files: If True, copies the input files before transforming. Ensures that the actual\n",
    "                input data remains untouched. Otherwise, it is directly altered.\n",
    "\n",
    "    Function only returns data when copy_files==True.\n",
    "    Note: Should also work for 3D, but has not been tested for that.\n",
    "    \"\"\"\n",
    "    for i,aug_file in enumerate(to_aug):\n",
    "        for ch in range(aug_file.shape[0]):\n",
    "            aug_file[ch,:]  = np.fliplr(aug_file[ch,:])\n",
    "\n",
    "    return to_aug\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "def vflip_2D(to_aug, rng=np.random.RandomState(1)):\n",
    "    \"\"\"\n",
    "    Perform standard 2D per-slice vertical flipping.\n",
    "    Arguments:\n",
    "    to_aug:     List of files that should be deformed in the same way. Each element\n",
    "                must be of standard Torch_Tensor shape: (N,C,W,H,...).\n",
    "                Deformation is done equally for each channel, but differently for\n",
    "                each image in a batch if N!=1.\n",
    "    rng:        Random Number Generator that can be provided for the Gaussian filter means.\n",
    "    copy_files: If True, copies the input files before transforming. Ensures that the actual\n",
    "                input data remains untouched. Otherwise, it is directly altered.\n",
    "\n",
    "    Function only returns data when copy_files==True.\n",
    "    Note: Should also work for 3D, but has not been tested for that.\n",
    "    \"\"\"\n",
    "    for i,aug_file in enumerate(to_aug):\n",
    "        for ch in range(aug_file.shape[0]):\n",
    "            aug_file[ch,:]  = np.flipud(aug_file[ch,:])\n",
    "\n",
    "    return to_aug\n",
    "\n",
    "def augment_2D(to_aug, mode_dict=[\"rot\",\"zoom\"], copy_files=False, return_files=False, seed=1, is_mask=[0,1,0]):\n",
    "    \"\"\"\n",
    "    Combine all augmentation methods to perform data augmentation (in 2D). Selection is done randomly.\n",
    "    Arguments:\n",
    "    to_aug:     List of files that should be deformed in the same way. Each element is a list with\n",
    "                Arrays of standard Torch_Tensor shape: (C,W,H,...).\n",
    "                Augmentation is done equally for each channel, but differently for\n",
    "                each image in a batch if N!=1.\n",
    "    mode_dict:  List of augmentation methods that should be used.\n",
    "    rng:        Random Number Generator that can be provided for the Gaussian filter means.\n",
    "    copy_files: If True, copies the input files before transforming. Ensures that the actual\n",
    "                input data remains untouched. Otherwise, it is directly altered.\n",
    "\n",
    "    Function only returns data when copy_files==True.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    modes = []\n",
    "\n",
    "    if rng.randint(2) and \"rot\" in mode_dict:\n",
    "        modes.append('rot')\n",
    "        to_aug, rotation_angle = rotate_2D(to_aug,rng)\n",
    "    if rng.randint(2) and \"zoom\" in mode_dict:\n",
    "        modes.append('zoom')\n",
    "        to_aug = zoom_2D(to_aug,rng)\n",
    "    if rng.randint(2) and \"hflip\" in mode_dict:\n",
    "        modes.append('hflip')\n",
    "        to_aug = hflip_2D(to_aug,rng)\n",
    "    if rng.randint(2) and \"vflip\" in mode_dict:\n",
    "        modes.append('vflip')\n",
    "        to_aug = vflip_2D(to_aug,rng)\n",
    "\n",
    "    return to_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=================================================\"\"\"\n",
    "\"\"\"============ Cropping for DataLoader ============\"\"\"\n",
    "\"\"\"=================================================\"\"\"\n",
    "def get_crops_per_batch(batch_to_crop, idx_batch=None, crop_size=[128,128], n_crops=1, seed=1):\n",
    "    \"\"\"\n",
    "    Function to crop from input images.\n",
    "    Takes as input a list of same-shaped 3D/4D-arrays with Ch,W,H(,D). If an index-file\n",
    "    is supplied, crops will only be taken in and around clusters in the index file. If the index-file\n",
    "    contains no clusters, then a random crop will be taken.\n",
    "\n",
    "    Arguments:\n",
    "    batch_to_crop:      list of batches that need to be cropped. Note that cropping is performed independently for\n",
    "                        each image of a batch.\n",
    "    idx_batch:          Batch of same size as input batches. Contains either clusters (i.e. ones) from which a\n",
    "                        cluster-center will be sampled or None. In this case, the center will be randomly selected.\n",
    "                        If not None, prov_coords must be None. The idx_image should ahve shape (1,W,H).\n",
    "    prov_coords:        If we have precomputed indices where we simply want to crop around, pass with prov_coords-argument.\n",
    "                        In this case, idx_batch must be None! When passed, prov_coords should be a list of lists/arrays containing\n",
    "                        the coordinate suggestions and should be of length batch_size!\n",
    "                        It is assumed that all cooridnates are already adjusted to viable ranges per volume.\n",
    "    crop_size:          Size of the crops to take -> len(crop_size) = input_batch.ndim-1, i.e. ignore batchdimension.\n",
    "    n_crops:            Number of crops to take per image. Ensure that this coincides with your chosen batchsize during training.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # assert (idx_batch is not None and prov_coords is None) or \\\n",
    "    #        (idx_batch is None and prov_coords is not None) or \\\n",
    "    #        (idx_batch is None and prov_coords is None), \"Error when passing arguments for idx_batch and/or prov_coords!\"\n",
    "    #\n",
    "    # assert all((np.array(batch_to_crop[0].shape[-len(crop_size):])-np.array(crop_size))>0), \"Crop size chosen to be bigger than volume!\"\n",
    "\n",
    "    sup = list(1-np.array(crop_size)%2)\n",
    "    bl_len = len(batch_to_crop)\n",
    "    batch_list_to_return = []\n",
    "\n",
    "    ### Provide idx-list\n",
    "    batch_list_to_return_temp = [[] for i in range(len(batch_to_crop))]\n",
    "\n",
    "    if idx_batch is not None:\n",
    "        all_crop_idxs = np.where(idx_batch[0,:]==1) if np.sum(idx_batch[0,:])!=0 else [[]]\n",
    "    else:\n",
    "        all_crop_idxs = [[]]\n",
    "\n",
    "    if len(all_crop_idxs[0]) > 0:\n",
    "        if idx_batch is not None:\n",
    "            crop_idx = [np.clip(rng.choice(ax),crop_size[i]//2-1,batch_to_crop[0][:].shape[i+1]-crop_size[i]//2-1) for i,ax in enumerate(all_crop_idxs)]\n",
    "    else:\n",
    "        crop_idx = [rng.randint(crop_size[i]//2-1,np.array(batch_to_crop[0].shape[i+1])-crop_size[i]//2-1) for i in range(batch_to_crop[0].ndim-1)]\n",
    "    # if prov_coords is not None:\n",
    "    # slice_list = [slice(0,None)]+[slice(center-crop_size[i]//2+mv,center+crop_size[i]//2+1) for i,(center,mv) in enumerate(zip(crop_idx,sup))]\n",
    "    # else:\n",
    "    \n",
    "    crop_coordinates = [(center - crop_size[i] // 2 + mv, center + crop_size[i] // 2 + 1) for i, (center, mv) in enumerate(zip(list(crop_idx),sup))]\n",
    "    \n",
    "    for i in range(bl_len):\n",
    "        batch_list_to_return.append(batch_to_crop[i][:, crop_coordinates[0][0]: crop_coordinates[0][1], crop_coordinates[1][0]: crop_coordinates[1][1]])\n",
    "\n",
    "    return tuple(batch_list_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========================================================================\"\"\"\n",
    "### CONFIG PARAMETERS\n",
    "\n",
    "liver_training_config = {\n",
    "    \"mode\": \"2D\",\n",
    "\t\"data\": \"liver\",\n",
    "\t\"n_epochs\": 50,\n",
    "\t\"lr\": 3e-05,\n",
    "\t\"l2_reg\": 1e-05,\n",
    "\t\"gpu\": 0,\n",
    "\t\"num_workers\": 8,\n",
    "\t\"batch_size\": 2,\n",
    "\t\"step_size\": [25, 42],\n",
    "\t\"gamma\": 0.1,\n",
    "\t\"crop_size\": [256, 256],\n",
    "\t\"perc_data\": 1,\n",
    "\t\"train_val_split\": 0.9,\n",
    "\t\"seed\": 1,\n",
    "\t\"loss_func\": \"multiclass_pwce\",\n",
    "\t\"class_weights\": [1, 1],\n",
    "\t\"num_classes\": 2,\n",
    "\t\"augment\": ['rot', 'zoom', 'hflip', 'vflip'],\n",
    "\t\"verbose_idx\": 200,\n",
    "\t\"initialization\": \"\",\n",
    "\t\"pos_sample_chance\": 2,\n",
    "\t\"no_standardize\": True,\n",
    "\t\"epsilon\": 1e-06,\n",
    "\t\"wmap_weight\": 3,\n",
    "\t\"weight_score\": [1, 1],\n",
    "\t\"focal_gamma\": 1.5,\n",
    "\t\"Training_ROI_Vicinity\": 4,\n",
    "\t\"savename\": \"liver_small\",\n",
    "\t\"use_weightmaps\": True,\n",
    "\t\"require_one_hot\": False,\n",
    "\t\"num_out_classes\": 2\n",
    "}\n",
    "\n",
    "lesion_training_config = {\n",
    "    \"mode\": \"2D\",\n",
    "\t\"data\": \"lesion\",\n",
    "\t\"n_epochs\": 50,\n",
    "\t\"lr\": 3e-05,\n",
    "\t\"l2_reg\": 1e-05,\n",
    "\t\"gpu\": 0,\n",
    "\t\"num_workers\": 8,\n",
    "\t\"batch_size\": 2,\n",
    "\t\"step_size\": [25, 42],\n",
    "\t\"gamma\": 0.1,\n",
    "\t\"crop_size\": [],\n",
    "\t\"perc_data\": 1,\n",
    "\t\"train_val_split\": 0.9,\n",
    "\t\"seed\": 1,\n",
    "\t\"loss_func\": \"multiclass_pwce\",\n",
    "\t\"class_weights\": [1, 1],\n",
    "\t\"num_classes\": 2,\n",
    "\t\"augment\": ['rot', 'zoom', 'hflip', 'vflip'],\n",
    "\t\"verbose_idx\": 200,\n",
    "\t\"initialization\": \"placeholder/SAVEDATA/Standard_Liver_Networks/vUnet2D_liver_small\",\n",
    "\t\"pos_sample_chance\": 2,\n",
    "\t\"no_standardize\": True,\n",
    "\t\"epsilon\": 1e-06,\n",
    "\t\"wmap_weight\": 3,\n",
    "\t\"weight_score\": [1, 1],\n",
    "\t\"focal_gamma\": 1.5,\n",
    "\t\"Training_ROI_Vicinity\": 4,\n",
    "\t\"savename\": \"liver_small\",\n",
    "\t\"use_weightmaps\": True,\n",
    "\t\"require_one_hot\": False,\n",
    "\t\"num_out_classes\": 2\n",
    "}\n",
    "\n",
    "\"\"\"=========================================================================\"\"\"\n",
    "### META FUNCTION TO RETURN ADJUSTED DATASETS, e.g. train-val-split --- 2D\n",
    "def Generate_Required_Datasets(config):\n",
    "    rng = np.random.RandomState(config['seed'])\n",
    "    vol_info = {}\n",
    "    vol_info['volume_slice_info'] = pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_Volumes.csv',     header=0)\n",
    "    vol_info['target_mask_info']  = pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_LesionMasks.csv', header=0) if config['data'] == 'lesion' else pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_LiverMasks.csv', header=0)\n",
    "\n",
    "    if config['data']=='lesion':  vol_info['ref_mask_info']     = pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_LiverMasks.csv', header=0)\n",
    "    if config['use_weightmaps']:  vol_info['weight_mask_info']  = pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_LesionWmaps.csv', header=0) if config['data'] == 'lesion' else pd.read_csv(ROOT_PREPROCESSED_TRAINING_DATA_PATH+'/Assign_2D_LiverWmaps.csv', header=0)\n",
    "\n",
    "    available_volumes = sorted(list(set(np.array(vol_info['volume_slice_info']['Volume']))), key=lambda x: int(x.split('-')[-1]))\n",
    "    rng.shuffle(available_volumes)\n",
    "\n",
    "    percentage_data_len = int(len(available_volumes)*config['perc_data'])\n",
    "    train_val_split     = int(percentage_data_len*config['train_val_split'])\n",
    "    training_volumes    = available_volumes[:percentage_data_len][:train_val_split]\n",
    "    validation_volumes  = available_volumes[:percentage_data_len][train_val_split:]\n",
    "\n",
    "\n",
    "    training_dataset   = Basic_Image_Dataset_2D(vol_info, training_volumes, config)\n",
    "    validation_dataset = Basic_Image_Dataset_2D(vol_info, validation_volumes, config, is_validation=True)\n",
    "    return training_dataset, validation_dataset\n",
    "\n",
    "\n",
    "\"\"\"=========================================================================\"\"\"\n",
    "### BASE DATASET CLASS IN 2D\n",
    "class Basic_Image_Dataset_2D(torch.utils.data.Dataset):\n",
    "    def __init__(self, vol_info, volumes, config, is_validation=False):\n",
    "        self.config = config\n",
    "\n",
    "        self.vol_info = vol_info\n",
    "\n",
    "        self.is_validation = is_validation\n",
    "\n",
    "        self.rng = np.random.RandomState(self.config[\"seed\"])\n",
    "\n",
    "        self.available_volumes = volumes\n",
    "\n",
    "        self.rvic = self.config[\"Training_ROI_Vicinity\"]\n",
    "\n",
    "        self.channel_size = 1\n",
    "\n",
    "        self.data_augmentation = True\n",
    "        if len(self.config[\"augment\"]) == 0 or self.is_validation:\n",
    "            self.data_augmentation = False\n",
    "\n",
    "        self.input_samples = {'Neg':[], 'Pos':[]}\n",
    "\n",
    "        self.div_in_volumes = {\n",
    "            key: {\n",
    "                    'Input_Image_Paths':[],\n",
    "                    'Has Target Mask':[],\n",
    "                    'Wmap_Paths':[],\n",
    "                    'TargetMask_Paths':[],\n",
    "                    'Has Ref Mask':[],\n",
    "                    'RefMask_Paths':[]\n",
    "                } for key in self.available_volumes\n",
    "            }\n",
    "        \n",
    "        # Record data paths for each slice in all training/validation volumes:\n",
    "        # * scan slice\n",
    "        # * object annotation flag (whether or not object class is visible in the scan slice/annotated scan slice)\n",
    "        # * weightmap\n",
    "        # * scan annotation\n",
    "        # * liver annotation flag (whether or not the liver is visible in the scan slice/annotated scan slice)\n",
    "        # * annotated liver scan slice\n",
    "        for i,vol in enumerate(vol_info['volume_slice_info']['Volume']):\n",
    "            if vol in self.div_in_volumes.keys():\n",
    "                self.div_in_volumes[vol]['Input_Image_Paths'].append(vol_info['volume_slice_info']['Slice Path'][i])\n",
    "                self.div_in_volumes[vol]['Has Target Mask'].append(vol_info['target_mask_info']['Has Mask'][i])\n",
    "                if self.config['use_weightmaps']: self.div_in_volumes[vol]['Wmap_Paths'].append(vol_info['weight_mask_info']['Slice Path'][i])\n",
    "                self.div_in_volumes[vol]['TargetMask_Paths'].append(vol_info['target_mask_info']['Slice Path'][i])\n",
    "                if self.config['data']=='lesion': self.div_in_volumes[vol]['Has Ref Mask'].append(vol_info['ref_mask_info']['Has Mask'][i])\n",
    "                if self.config['data']=='lesion': self.div_in_volumes[vol]['RefMask_Paths'].append(vol_info['ref_mask_info']['Slice Path'][i])\n",
    "\n",
    "        self.volume_details = {\n",
    "            key: {\n",
    "                    'Input_Image_Paths':[],\n",
    "                    'TargetMask_Paths':[],\n",
    "                    'Wmap_Paths':[],\n",
    "                    'RefMask_Paths':[]\n",
    "                } for key in self.available_volumes\n",
    "            }\n",
    "\n",
    "        # Populate dictionary with all necessary data for training\n",
    "        for i,vol in enumerate(self.div_in_volumes.keys()):\n",
    "            for j in range(len(self.div_in_volumes[vol]['Input_Image_Paths'])):\n",
    "                crop_condition = np.sum(self.div_in_volumes[vol]['Has Ref Mask'][int(np.clip(j-self.rvic, 0, None)):j+self.rvic])\n",
    "                if self.config['data']=='liver': crop_condition=True\n",
    "\n",
    "                if crop_condition:\n",
    "                    extra_ch = self.channel_size//2\n",
    "                    low_bound, low_diff = np.clip(j-extra_ch,0,None).astype(int), extra_ch-j\n",
    "                    up_bound, up_diff = np.clip(j+extra_ch+1,None,len(self.div_in_volumes[vol][\"Input_Image_Paths\"])).astype(int), j+extra_ch+1-len(self.div_in_volumes[vol][\"Input_Image_Paths\"])\n",
    "\n",
    "                    vol_slices = self.div_in_volumes[vol][\"Input_Image_Paths\"][low_bound:up_bound]\n",
    "\n",
    "                    if low_diff>0:\n",
    "                        extra_slices = self.div_in_volumes[vol][\"Input_Image_Paths\"][low_bound+1:low_bound+1+low_diff][::-1]\n",
    "                        vol_slices = extra_slices+vol_slices\n",
    "                    if up_diff>0:\n",
    "                        extra_slices = self.div_in_volumes[vol][\"Input_Image_Paths\"][up_bound-up_diff-1:up_bound-1][::-1]\n",
    "                        vol_slices = vol_slices+extra_slices\n",
    "\n",
    "                    self.volume_details[vol]['Input_Image_Paths'].append(vol_slices)\n",
    "                    self.volume_details[vol]['TargetMask_Paths'].append(self.div_in_volumes[vol]['TargetMask_Paths'][j])\n",
    "\n",
    "                    if self.config['data']!='liver':  self.volume_details[vol]['RefMask_Paths'].append(self.div_in_volumes[vol]['RefMask_Paths'][j])\n",
    "                    if self.config['use_weightmaps']: self.volume_details[vol]['Wmap_Paths'].append(self.div_in_volumes[vol]['Wmap_Paths'][j])\n",
    "\n",
    "                    type_key = 'Pos' if self.div_in_volumes[vol]['Has Target Mask'][j] or self.is_validation else 'Neg'\n",
    "                    self.input_samples[type_key].append((vol, len(self.volume_details[vol]['Input_Image_Paths'])-1))\n",
    "\n",
    "        self.n_files  = np.sum([len(self.input_samples[key]) for key in self.input_samples.keys()])\n",
    "        self.curr_vol = self.input_samples['Pos'][0][0] if len(self.input_samples['Pos']) else self.input_samples['Neg'][0][0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Choose a positive example with 50% change if training.\n",
    "        #During validation, 'Pos' will contain all validation samples.\n",
    "        #Note that again, volumes without lesions/positive target masks need to be taken into account.\n",
    "        type_choice = not idx % self.config['pos_sample_chance'] or self.is_validation\n",
    "        modes       = list(self.input_samples.keys())\n",
    "        type_key    = modes[type_choice] if len(self.input_samples[modes[type_choice]]) else modes[not type_choice]\n",
    "    \n",
    "        type_len = len(self.input_samples[type_key])\n",
    "\n",
    "        vol, idx   = self.input_samples[type_key][idx%type_len]\n",
    "        next_vol,_ = self.input_samples[type_key][(idx+1)%type_len]\n",
    "\n",
    "        vol_change = next_vol!=vol\n",
    "        self.curr_vol   = vol\n",
    "    \n",
    "        intvol = self.volume_details[vol][\"Input_Image_Paths\"][idx]\n",
    "        intvol = intvol[len(intvol)//2]\n",
    "\n",
    "        input_image  = np.concatenate([np.expand_dims(np.load(sub_vol),0) for sub_vol in self.volume_details[vol][\"Input_Image_Paths\"][idx]],axis=0)\n",
    "\n",
    "        #Perform data standardization\n",
    "        if self.config['no_standardize']:\n",
    "            input_image  = normalize(input_image, zero_center=False, unit_variance=False, supply_mode=\"orig\")\n",
    "        else:\n",
    "            input_image  = normalize(input_image)\n",
    "\n",
    "        #Lesion/Liver Mask to output\n",
    "        target_mask = np.load(self.volume_details[vol][\"TargetMask_Paths\"][idx])\n",
    "        target_mask = np.expand_dims(target_mask,0)\n",
    "\n",
    "\n",
    "        #Liver Mask to use for defining training region of interest\n",
    "        crop_mask = np.expand_dims(np.load(self.volume_details[vol][\"RefMask_Paths\"][idx]),0) if self.config['data']=='lesion' else None\n",
    "        #Weightmask to output\n",
    "        weightmap = np.expand_dims(np.load(self.volume_details[vol][\"Wmap_Paths\"][idx]),0).astype(float) if self.config['use_weightmaps'] else None\n",
    "\n",
    "\n",
    "        #Generate list of all files that would need to be crop, if cropping is required.\n",
    "        files_to_crop  = [input_image, target_mask]\n",
    "        is_mask        = [0,1]\n",
    "        if weightmap is not None:\n",
    "            files_to_crop.append(weightmap)\n",
    "            is_mask.append(0)\n",
    "        if crop_mask is not None:\n",
    "            files_to_crop.append(crop_mask)\n",
    "            is_mask.append(1)\n",
    "\n",
    "        #First however, augmentation, if required, is performed (i.e. on fullsize images to remove border artefacts in crops).\n",
    "        if self.data_augmentation:\n",
    "            files_to_crop = list(augment_2D(files_to_crop, mode_dict = self.config[\"augment\"],\n",
    "                                               seed=self.rng.randint(0,1e8), is_mask = is_mask))\n",
    "\n",
    "        #If Cropping is required, we crop now.\n",
    "        if len(self.config['crop_size']) and not self.is_validation:\n",
    "            #Add imaginary batch axis in gu.get_crops_per_batch\n",
    "            crops_for_picked_batch  = get_crops_per_batch(files_to_crop, crop_mask, crop_size=self.config['crop_size'], seed=self.rng.randint(0,1e8))\n",
    "            input_image     = crops_for_picked_batch[0]\n",
    "            target_mask     = crops_for_picked_batch[1]\n",
    "            weightmap       = crops_for_picked_batch[2] if weightmap is not None else None\n",
    "            crop_mask       = crops_for_picked_batch[-1] if crop_mask is not None else None\n",
    "\n",
    "\n",
    "        # #If a one-hot encoded target mask is required:\n",
    "        # one_hot_target = gu.numpy_generate_onehot_matrix(target_mask, self.pars.Training['num_classes']) if self.pars.Training['require_one_hot'] else None\n",
    "\n",
    "        # #If we use auxiliary inputs to input additional information into the network, we compute respective outputs here.\n",
    "        # auxiliary_targets, auxiliary_wmaps, one_hot_auxiliary_targets   = None, None, None\n",
    "        # if not self.is_validation and self.pars.Network['use_auxiliary_inputs']:\n",
    "        #     auxiliary_targets, auxiliary_wmaps, one_hot_auxiliary_targets   = [], [], []\n",
    "        #     for val in range(len(self.pars.Network['structure'])-1):\n",
    "        #         aux_level = 2**(val+1)\n",
    "        #         aux_img = np.round(st.resize(target_mask,(target_mask.shape[0], target_mask.shape[1]//aux_level,target_mask.shape[2]//aux_level),order=0, mode=\"reflect\", preserve_range=True))\n",
    "        #         auxiliary_targets.append(aux_img)\n",
    "        #         if self.pars.Training['require_one_hot']:\n",
    "        #             one_hot_auxiliary_targets.append(gu.numpy_generate_onehot_matrix(aux_img, self.pars.Training['num_classes']))\n",
    "        #         if weightmap is not None:\n",
    "        #             aux_img = st.resize(weightmap,(weightmap.shape[0], weightmap.shape[1]//aux_level,weightmap.shape[2]//aux_level),order=0, mode=\"reflect\", preserve_range=True)\n",
    "        #             auxiliary_wmaps.append(aux_img)\n",
    "\n",
    "        one_hot_target = None\n",
    "        auxiliary_targets = None\n",
    "        one_hot_auxiliary_targets = None\n",
    "        auxiliary_wmaps = None\n",
    "\n",
    "        #Final Output Dictionary\n",
    "        return_dict = {\"input_images\":input_image.astype(float), \"targets\":target_mask.astype(float),\n",
    "                       \"crop_option\":crop_mask.astype(float) if crop_mask is not None else None,\n",
    "                       \"weightmaps\":weightmap.astype(float) if weightmap is not None else None,\n",
    "                       \"one_hot_targets\":one_hot_target,\n",
    "                       \"aux_targets\":auxiliary_targets, \"one_hot_aux_targets\": one_hot_auxiliary_targets,\n",
    "                       \"aux_weightmaps\": auxiliary_wmaps, 'internal_slice_name':intvol, 'vol_change':vol_change}\n",
    "\n",
    "        return_dict = {key:item for key,item in return_dict.items() if item is not None}\n",
    "        return return_dict\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = Generate_Required_Datasets(liver_training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, num_workers=liver_training_config['num_workers'], batch_size=liver_training_config['batch_size'], pin_memory=False, shuffle=True)\n",
    "val_data_loader   = torch.utils.data.DataLoader(val_dataset,   num_workers=0, batch_size=1, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
